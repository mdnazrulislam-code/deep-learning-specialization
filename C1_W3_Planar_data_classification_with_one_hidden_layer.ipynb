{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1biTwd128PoTjveONmgBQQQoZtDlfiiHh",
      "authorship_tag": "ABX9TyMk0PeDlDe29yR7Pp2+iBhL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoYEPYZ1Ex1J"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip data in google drive...\n",
        "!unzip Dataset_Path/planar_dataset.zip -d Dataset_Path/Datasets/"
      ],
      "metadata": {
        "id": "Q4D77Z4xFtdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyreadr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEm4cBp9Gb8n",
        "outputId": "eccef09a-7276-4e5a-bad0-55e65cdef9d0"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyreadr in /usr/local/lib/python3.8/dist-packages (0.4.7)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyreadr) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyreadr) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyreadr) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "import pyreadr\n",
        "result = pyreadr.read_r('Dataset_Path/planar_dataset.rds') # also works for RData\n"
      ],
      "metadata": {
        "id": "Tvu_dvKuGSJh"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset according to the column\n",
        "df1 = result[None][\"x1\"]\n",
        "df2 = result[None][\"x2\"]\n",
        "df3 = result[None][\"y\"]\n",
        "D = list()\n",
        "for i in range(0,len(df1)):\n",
        "  D.append((df1[i],df2[i]))\n",
        "X = np.array(D) \n",
        "Y = np.array(df3)\n",
        "Y = Y.reshape(Y.shape[0],1)"
      ],
      "metadata": {
        "id": "P1nPV2yWGhn3"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transpose the data\n",
        "X = X.T\n",
        "Y = Y.T\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqDubayPHmBQ",
        "outputId": "03af09fe-06f1-41b7-8d0b-19e68e416f8b"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 400)\n",
            "(1, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: layer_sizes\n",
        "\n",
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    #(≈ 3 lines of code)\n",
        "    # n_x = ... \n",
        "    # n_h = ...\n",
        "    # n_y = ... \n",
        "    # YOUR CODE STARTS HERE\n",
        "    n_x = X.shape[0]\n",
        "    n_h = 4\n",
        "    n_y = Y.shape[0]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return (n_x, n_h, n_y)"
      ],
      "metadata": {
        "id": "15jjb-PLNSQI"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"    \n",
        "    #(≈ 4 lines of code)\n",
        "    # W1 = ...\n",
        "    # b1 = ...\n",
        "    # W2 = ...\n",
        "    # b2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    # YOUR CODE ENDS HERE\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "Y6_7GL4UR-Ev"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    return A"
      ],
      "metadata": {
        "id": "Y6QVk_dPTHBP"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION:forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    #(≈ 4 lines of code)\n",
        "    # W1 = ...\n",
        "    # b1 = ...\n",
        "    # W2 = ...\n",
        "    # b2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    # (≈ 4 lines of code)\n",
        "    # Z1 = ...\n",
        "    # A1 = ...\n",
        "    # Z2 = ...\n",
        "    # A2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    Z1 = np.dot(W1,X)+b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2,A1)+b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    # YOUR CODE ENDS HERE\n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    return A2, cache"
      ],
      "metadata": {
        "id": "c9igvproSEdg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: compute_cost\n",
        "\n",
        "def compute_cost(A2, Y):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \n",
        "    \"\"\"\n",
        "    m = Y.shape[1] # number of examples\n",
        "    # Compute the cross-entropy cost\n",
        "    # (≈ 2 lines of code)\n",
        "    # logprobs = ...\n",
        "    # cost = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    logprobs = np.multiply(np.log(A2),Y)+ np.multiply((1-Y),np.log(1-A2))\n",
        "    cost = (-1/m)*np.sum(logprobs)   \n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
        "                                    # E.g., turns [[17]] into 17 \n",
        "    return cost"
      ],
      "metadata": {
        "id": "FgPDReCCTtw4"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    #(≈ 2 lines of code)\n",
        "    # W1 = ...\n",
        "    # W2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    #(≈ 2 lines of code)\n",
        "    # A1 = ...\n",
        "    # A2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    A1 = cache[\"A1\"]\n",
        "    Z1 = cache[\"Z1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    Z2 = cache[\"Z2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
        "    # dZ2 = ...\n",
        "    # dW2 = ...\n",
        "    # db2 = ...\n",
        "    # dZ1 = ...\n",
        "    # dW1 = ...\n",
        "    # db1 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
        "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
        "    dZ1 = np.dot(W2.T,dZ2)*(1 - np.power(A1, 2))\n",
        "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
        "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    return grads"
      ],
      "metadata": {
        "id": "uUjF-l0aUyfn"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n",
        "    #(≈ 4 lines of code)\n",
        "    # W1 = ...\n",
        "    # b1 = ...\n",
        "    # W2 = ...\n",
        "    # b2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    #(≈ 4 lines of code)\n",
        "    # dW1 = ...\n",
        "    # db1 = ...\n",
        "    # dW2 = ...\n",
        "    # db2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    #(≈ 4 lines of code)\n",
        "    # W1 = ...\n",
        "    # b1 = ...\n",
        "    # W2 = ...\n",
        "    # b2 = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    W1 = W1-learning_rate*dW1\n",
        "    b1 = b1-learning_rate*db1\n",
        "    W2 = W2-learning_rate*dW2\n",
        "    b2 = b2-learning_rate*db2\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "9PxZNocxUzg3"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    #(≈ 1 line of code)\n",
        "    # parameters = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        #(≈ 4 lines of code)\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        # A2, cache = ...\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
        "        # cost = ...\n",
        "        cost = compute_cost(A2, Y)\n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        # grads = ...\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        # parameters = ...\n",
        "        parameters = update_parameters(parameters, grads, learning_rate=1.2)\n",
        "        # YOUR CODE STARTS HERE\n",
        "        \n",
        "        \n",
        "        # YOUR CODE ENDS HERE\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "zospEccbVQ1N"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = nn_model(X, Y, n_h = 4, num_iterations = 20000, print_cost=True)\n",
        "\n",
        "# Plot the decision boundary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XHhc-nHVxSY",
        "outputId": "f373c4c7-39d9-4106-fd65-91433fc705e1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693165\n",
            "Cost after iteration 1000: 0.241590\n",
            "Cost after iteration 2000: 0.228501\n",
            "Cost after iteration 3000: 0.223446\n",
            "Cost after iteration 4000: 0.220356\n",
            "Cost after iteration 5000: 0.218124\n",
            "Cost after iteration 6000: 0.216372\n",
            "Cost after iteration 7000: 0.214926\n",
            "Cost after iteration 8000: 0.214053\n",
            "Cost after iteration 9000: 0.212706\n",
            "Cost after iteration 10000: 0.211737\n",
            "Cost after iteration 11000: 0.210856\n",
            "Cost after iteration 12000: 0.210046\n",
            "Cost after iteration 13000: 0.209297\n",
            "Cost after iteration 14000: 0.208601\n",
            "Cost after iteration 15000: 0.207951\n",
            "Cost after iteration 16000: 0.207344\n",
            "Cost after iteration 17000: 0.206775\n",
            "Cost after iteration 18000: 0.206242\n",
            "Cost after iteration 19000: 0.205740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    #(≈ 2 lines of code)\n",
        "    # A2, cache = ...\n",
        "    # predictions = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = (A2 > 0.5)\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Z-rZMIlXWihu"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLYEv9UzXAmX",
        "outputId": "1841026b-1473-4f29-dce2-42c926a397eb"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = predictions.reshape(400)\n",
        "actual = Y.reshape(400)"
      ],
      "metadata": {
        "id": "QddS2dWdbLJ_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "cm = tf.math.confusion_matrix(labels=actual,predictions=predicted)\n",
        "import seaborn as sn\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "mmzHIKu4baWY",
        "outputId": "640eded4-e907-4dd3-be8f-de506b4cf993"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(69.0, 0.5, 'Truth')"
            ]
          },
          "metadata": {},
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGpCAYAAACam6wDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf9UlEQVR4nO3debQdVbWo8W9CBASRAIEYE7xEaTR2yEVEuSgKKmAT9fm8oAgP0aMSReEq2KNiA4pyUdRrhEhQDCCixl5EIaAGiEjfaESBRCRwaSSgaHLm++NUcBNymhx2V7W+H6NGqlbVXrV2GGecmTnXqorMRJIkqc7W6fUAJEmSHi4DGkmSVHsGNJIkqfYMaCRJUu0Z0EiSpNqb0OsBDOf+Pyx0+ZXUA495+v69HoJUrDuXL45u3u+ft9/Qtt+1j5j0+K6OfXVmaCRJUu31bYZGkiR12ODKXo+gbczQSJKk2jNDI0lSqXKw1yNoGwMaSZJKNdicgMaSkyRJqj0zNJIkFSotOUmSpNqz5CRJktQ/zNBIklQqS06SJKn2fLCeJElS/zCgkSSpVDnYvm0UETEnIpZFxFUtbTtExMKIuCwiFkXEzlV7RMTnImJxRFwRETuO1r8BjSRJpRocbN82ulOAvVZr+xTwkczcAfhQdQywN7BttQ0AXxqtcwMaSZLUcZm5ALhj9Wbg0dX+JsCfq/2ZwKk5ZCEwMSKmjNS/k4IlSSpUOx+sFxEDDGVTVpmdmbNH+dg7gZ9ExHEMJVmeU7VPBW5uuW5J1XbLcB0Z0EiSVKo2PlivCl5GC2BW91bgsMz8VkS8BjgZ2HM897fkJEmSeuVA4Oxq/5vAztX+UmCrluumVW3DMqCRJKlUXVzlNIw/A8+r9l8A/L7anw8cUK122gW4OzOHLTeBJSdJksrVxQfrRcQ8YHdgUkQsAY4C3gScEBETgL/zrzk4PwT2ARYD9wEHjda/AY0kSeq4zNxvmFP/voZrE5i1Nv0b0EiSVCrf5SRJkmqvjauces1JwZIkqfbM0EiSVCpLTpIkqfYsOUmSJPUPMzSSJBUqs3vPoek0AxpJkkrVoDk0lpwkSVLtmaGRJKlUDZoUbEAjSVKpGlRyMqCRJKlUXXw5Zac5h0aSJNWeGRpJkkplyUmSJNVegyYFW3KSJEm1Z4ZGkqRSWXKSJEm1Z8lJkiSpf5ihkSSpVA3K0BjQSJJUqCa9bduSkyRJqj0zNJIklcqSkyRJqr0GLdu25CRJkmrPDI0kSaWy5CRJkmrPkpMkSVL/MEMjSVKpLDlJkqTas+QkSZLUP8zQSJJUKktOkiSp9hoU0FhykiRJtWdAI0lSqXKwfdsoImJORCyLiKtWa397RFwXEVdHxKda2t8bEYsj4vqIePFo/VtykiSpVN0tOZ0CnAicuqohIp4PzASenpn3R8SWVfsMYF/gycBjgZ9FxHaZuXK4zs3QSJKkjsvMBcAdqzW/FTgmM++vrllWtc8ETs/M+zPzj8BiYOeR+jegkSSpVG0sOUXEQEQsatkGxjCC7YDdIuKiiDg/Ip5ZtU8Fbm65bknVNixLTpIklaqNJafMnA3MXsuPTQA2A3YBngmcGRGPH8/9zdBIkqReWQKcnUMuBgaBScBSYKuW66ZVbcMyoJEkqVRdXOU0jO8AzweIiO2A9YDbgfnAvhGxfkRMB7YFLh6pI0tOkiSVqournCJiHrA7MCkilgBHAXOAOdVS7n8AB2ZmAldHxJnANcAKYNZIK5zAgEaSJHVBZu43zKn9h7n+48DHx9q/AY0kSaVq0KsPDGgkSSpVZq9H0DZOCpYkSbVnhkaSpFJZcpIkSbXXoIDGkpMkSao9MzSSJJVq/A/E6zsGNJIklcqSkyRJUv8wQyNJUqka9BwaAxpJkkplyUmSJKl/mKGRJKlUDcrQGNBIklSqBi3btuQkSZJqzwyNJEmFykFXOUmSpLpr0BwaS06SJKn2zNBIklSqBk0KNqCRJKlUDZpDY8lJkiTVnhkaSZJK1aBJwQY0kiSVyoBGkiTVXoPetu0cGkmSVHtmaCRJKpUlJ5XsQ8efxPkXX8ZmEx/Nt7/0CQCu+8ONHH3iXP7xz3+y7jrr8P5ZB/DU7Z9AZnLsl0/jgksuZ4P11+Pow9/EjG227u0XkBpg6tQpfOkrn2aLLSeRmcz96ul8+YtzmbjpJsyZewKPe9w0brppCQcdcCh33/XXXg9X/cpl2yrZy/f8D7509Lse1Hb8nDN4y2tn8s0Tj2bW61/F8XPOBODCRVdw49K/8P2TPsWHDj2Ij504txdDlhpnxYoVfOC9n+TZO+3Fi57/at74pv3Z/onbcNjhb2bBeb9mpx32ZMF5v+aww9/c66FKXWFAo7W201OfyCYbb/Sgtojg3vv+DsA9997HFptNBOAXCy/lZXvsSkTw9Cduwz333sdtd9zV9TFLTXPrrbdxxeVXA7B8+b387vo/MGXKZPZ+yZ7MO+1sAOaddjb7vPSFvRym+l0Otm/rsY6VnCLiicBMYGrVtBSYn5nXduqe6p0jBl7HWz74aT5z8ulkDnLqcR8EYNntd/KYLTZ/4LrJkzZj2e13PhDwSHr4tnrcVJ729Bn8ZtHlbLnlJG699TZgKOjZcstJPR6d+polp5FFxJHA6UAAF1dbAPMi4j0jfG4gIhZFxKKTTv9OJ4amDjnzhz/n3W96LeecejzvftNrOeqEk3s9JKkIG220Iaee9gXee+THuOee5Q85nw1aliuNpFMZmoOBJ2fmP1sbI+KzwNXAMWv6UGbOBmYD3P+Hhf4U1sj8n13IkW9+HQAv2m1nPnzCHAC2nLQpf7ntfx+47tbb72DLSZv2ZIxS00yYMIG5p32Bb54xn+/P/ykAy5bdzuTJW3DrrbcxefIW3Nby8yetLhu0yqlTc2gGgceuoX1KdU4Ns8XmE1l05XUAXHT5NTxu6mQAdn/WM/jeub8kM7n8usVsvNEjLTdJbfL5L36S312/mC+eOOeBth//8Fz2e92rANjvda/iRz/4Wa+GpzoYzPZtPRadSEdGxF7AicDvgZur5scB2wBvy8wfj9aHGZr+dcSxX2TRFddx11+Xs9nER3PI/q9k62lTOPbLX2flykHWe8Qj+MCsA5ix7XQyk0988Wv88jdXsMH663P0YW/kydtN7/VX0Age8/T9ez0EjcEuz/53fnTOGVx91XUMVv/KPvrDn2HRosv56qmfY9q0x3LzzUs56IBDuevOu3s8Wo3VncsXRzfvd+/HD2jb79qN3n9qV8e+uo4ENAARsQ6wMw+eFHxJZq4cy+cNaKTeMKCReqfrAc3H9m9fQPOBr/c0oOnYsu3MHMzMhZn5rWpbONZgRpIkdUEXS04RMScilkXEVWs4918RkRExqTqOiPhcRCyOiCsiYsfR+vc5NJIkqRtOAfZavTEitgJeBNzU0rw3sG21DQBfGq1zAxpJkko1ONi+bRSZuQC4Yw2njgeOAFrTPDOBU3PIQmBiREwZqX8DGkmSStXGklPrs+SqbWC020fETGBpZl6+2qmp/GtREcAS/jUnd418OaUkSXrYWp8lNxYRsSHwPobKTQ+bAY0kSaXq7TuYngBMBy6PCIBpwKURsTNDK6O3arl2WtU2LAMaSZJK1cMH4mXmlcCWq44j4k/ATpl5e0TMB94WEacDzwLuzsxbRurPOTSSJKnjImIe8Gtg+4hYEhEHj3D5D4EbgMXAV4BDRuvfDI0kSYXq5rucMnO/Uc5v3bKfwKy16d+ARpKkUvXBO5jaxZKTJEmqPTM0kiSVqkEZGgMaSZJK1dtl221lyUmSJNWeGRpJkkplyUmSJNVdNiigseQkSZJqzwyNJEmlalCGxoBGkqRSdfFJwZ1myUmSJNWeGRpJkkplyUmSJNVegwIaS06SJKn2zNBIklSozOZkaAxoJEkqlSUnSZKk/mGGRpKkUjUoQ2NAI0lSoXyXkyRJUh8xQyNJUqkalKExoJEkqVTNeZWTJSdJklR/ZmgkSSpUkyYFG9BIklSqBgU0lpwkSVLtmaGRJKlUDZoUbEAjSVKhmjSHxpKTJEmqPTM0kiSVypKTJEmqO0tOkiRJfcQMjSRJpbLkJEmS6i4NaCRJUu01KKBxDo0kSeq4iJgTEcsi4qqWtk9HxHURcUVEfDsiJrace29ELI6I6yPixaP1b0AjSVKhcrB92xicAuy1Wts5wFMy82nA74D3AkTEDGBf4MnVZ74YEeuO1LkBjSRJpRps4zaKzFwA3LFa208zc0V1uBCYVu3PBE7PzPsz84/AYmDnkfo3oJEkSQ9bRAxExKKWbWAtu3gD8KNqfypwc8u5JVXbsJwULElSodq5yikzZwOzx/PZiHg/sAI4bbz3N6CRJKlQ/bBsOyL+H/BSYI/MXPXo4qXAVi2XTavahmXJSZIk9URE7AUcAbw8M+9rOTUf2Dci1o+I6cC2wMUj9WWGRpKkQnUzQxMR84DdgUkRsQQ4iqFVTesD50QEwMLMfEtmXh0RZwLXMFSKmpWZK0fq34BGkqRSZXTvVpn7raH55BGu/zjw8bH2b8lJkiTVnhkaSZIK1Q+TgtvFgEaSpELlYPdKTp1myUmSJNWeGRpJkgplyUmSJNVednGVU6dZcpIkSbVnhkaSpEJZcpIkSbXnKidJkqQ+YoZGkqRCPfBu6wYwoJEkqVCWnCRJkvqIGRpJkgrVpAyNAY0kSYVq0hwaS06SJKn2zNBIklQoS06SJKn2fJeTJElSHzFDI0lSoXyXkyRJqr1BS06SJEn9wwyNJEmFatKkYAMaSZIK1aRl25acJElS7ZmhkSSpUE169YEBjSRJhWpSyWlMAU1EPAfYuvX6zDy1Q2OSJElaK6MGNBHxNeAJwGXAyqo5AQMaSZJqrEnPoRlLhmYnYEZmkyptkiSpScu2x7LK6SrgMZ0eiCRJ0ngNm6GJiO8xVFraGLgmIi4G7l91PjNf3vnhSZKkTmlS7WWkktNxXRuFJEnquiLm0GTm+QARcWxmHtl6LiKOBc7v8NgkSZLGZCxzaF64hra92z0QSZLUXZnRtm00ETEnIpZFxFUtbZtFxDkR8fvqz02r9oiIz0XE4oi4IiJ2HK3/YQOaiHhrRFwJPLHqbNX2R+DKMf1NSZKkvpXZvm0MTgH2Wq3tPcC5mbktcG51DEOJk22rbQD40midjzSH5hvAj4BPttwA4J7MvGMsI5ckSQLIzAURsfVqzTOB3av9ucB5wJFV+6nVI2MWRsTEiJiSmbcM1/9Ic2juBu6OiCNXO/WoiHhUZt60Nl9kbW30pP/Tye4lDeNvf76g10OQ1CV9MCl4ckuQ8hdgcrU/Fbi55bolVdvaBzQtfsDQ8u0ANgCmA9cDT167MUuSpH7SzgfrRcQAQ+WhVWZn5uyxjyUzIsa9kHzUgCYzn9p6XE3MOWS8N5QkSc1TBS9jDmAqt64qJUXEFGBZ1b4U2KrlumlV27DGssrpQTLzUuBZa/s5SZLUXwYz2raN03zgwGr/QOC7Le0HVKuddgHuHmn+DIzt5ZSHtxyuA+wI/HmthyxJkvpKNx8UHBHzGJoAPCkilgBHAccAZ0bEwcCNwGuqy38I7AMsBu4DDhqt/7HModm4ZX8FQ3NqvjXG8UuSpD7VzUnBmbnfMKf2WMO1Ccxam/5HDGgiYl1g48x819p0KkmS1E0jvZxyQmauiIhduzkgSZLUHe1c5dRrI2VoLmZovsxlETEf+CZw76qTmXl2h8cmSZI6aLDXA2ijscyh2QD4X+AF/Ot5NAkY0EiSpL4wUkCzZbXC6Sr+Fcis0s2J0ZIkqQOSMkpO6wKPgjV+WwMaSZJqbrBBv81HCmhuycyPdm0kkiRJ4zRSQNOcPJQkSXqIwQb9qh8poHnIg24kSVJzNGkOzbDvcsrMO7o5EEmSpPEay7JtSZLUQKU9h0aSJDVQESUnSZKkujBDI0lSoSw5SZKk2mtSQGPJSZIk1Z4ZGkmSCtWkScEGNJIkFWqwOfGMJSdJklR/ZmgkSSpUKe9ykiRJDZa9HkAbWXKSJEm1Z4ZGkqRCNek5NAY0kiQVajCaM4fGkpMkSao9MzSSJBWqSZOCDWgkSSpUk+bQWHKSJEm1Z4ZGkqRCNenVBwY0kiQVqklPCrbkJEmSas8MjSRJhXKVkyRJqr0mzaGx5CRJkmrPDI0kSYXyOTSSJKn2so3baCLisIi4OiKuioh5EbFBREyPiIsiYnFEnBER6433uxjQSJKkjoqIqcChwE6Z+RRgXWBf4Fjg+MzcBrgTOHi89zCgkSSpUIPRvm0MJgCPjIgJwIbALcALgLOq83OBV4z3uxjQSJJUqME2bhExEBGLWraBVffJzKXAccBNDAUydwO/Ae7KzBXVZUuAqeP9Lk4KliRJD1tmzgZmr+lcRGwKzASmA3cB3wT2auf9DWgkSSpUF1c57Qn8MTNvA4iIs4FdgYkRMaHK0kwDlo73BpacJEkqVEb7tlHcBOwSERtGRAB7ANcAvwBeXV1zIPDd8X4XAxpJktRRmXkRQ5N/LwWuZCj+mA0cCRweEYuBzYGTx3sPS06SJBWqmw/Wy8yjgKNWa74B2Lkd/RvQSJJUKJ8ULEmS1EfM0EiSVKixvLKgLgxoJEkq1Bif8FsLlpwkSVLtmaGRJKlQTZoUbEAjSVKhmhTQWHKSJEm1Z4ZGkqRCucpJkiTVXpNWORnQSJJUKOfQSJIk9REzNJIkFco5NJIkqfYGGxTSWHKSJEm1Z4ZGkqRCNWlSsAGNJEmFak7ByZKTJElqADM0kiQVypKTJEmqvSY9KdiSkyRJqj0zNJIkFapJz6ExoJEkqVDNCWcsOUmSpAYwQyNJUqFc5SRJkmqvSXNoLDlJkqTaM0MjSVKhmpOfMaCRJKlYTZpDY8lJkiTVnhkaSZIK1aRJwQY0kiQVqjnhjCUnSZLUAGZoJEkqVJMmBRvQSJJUqGxQ0cmSkyRJ6riImBgRZ0XEdRFxbUQ8OyI2i4hzIuL31Z+bjrd/AxpJkgo12MZtDE4AfpyZTwSeDlwLvAc4NzO3Bc6tjsfFgEaSpEINkm3bRhIRmwDPBU4GyMx/ZOZdwExgbnXZXOAV4/0uBjSSJOlhi4iBiFjUsg20nJ4O3AZ8NSJ+GxEnRcRGwOTMvKW65i/A5PHe30nBkiQVqp1TgjNzNjB7mNMTgB2Bt2fmRRFxAquVlzIzI2LcQzJDI0lSobpVcgKWAEsy86Lq+CyGApxbI2IKQPXnsvF+FwMaSZLUUZn5F+DmiNi+atoDuAaYDxxYtR0IfHe897DkpIftK7M/w0v22ZNlt93ODs/YA4CPfPjdvOxlL2JwMLlt2e284Y2Hccstt/Z4pFL9feATn2XBLy9ms00n8p2v/w8A1/3uD3z005/n/n/8k3XXXZcPvmsWT52xPXNOO4sf/PQXAKxcuZIbbryZC35wOps8euNefgX1kS4/WO/twGkRsR5wA3AQQ4mVMyPiYOBG4DXj7Twy+/OhOhPWm9qfA9ND7PYfz2L58nv56ldPeCCg2XjjR3HPPcsBeNusN/CkJ23HrLeNezWeuuhvf76g10PQCBZddiUbPvKRvO/o4x4IaN70zvdxwH++kt2e/UwW/Opi5nzjLE458VMP+tx5Fy7k1DO+w5zPH9OLYWuMHjHp8dHN+71x61e37XftSX86q6tjX50lJz1sF1x4EXfcedeD2lYFMwAbbbQh/Ro4S3Wz0w5PfUiGJSJYfu99ACy/9z62nLT5Qz73w5+dzz4vfF5Xxij1giUndczRHz2S/V/3au7+61/Z84X/t9fDkRrryHe8mTcf/gGO+8JJ5GDy9S9/5kHn//b3v3PhwkW8//BDejRC9asmvcup6xmaiDhohHMPrGEfHLy3m8NSB3zwQ8cy/QnPZN68bzPrkGH/t0t6mM749g848u0DnPvtr3HEoQN86JP//aDz5114Ec942gznzughso3/9VovSk4fGe5EZs7OzJ0yc6d11tmom2NSB31j3tm88pX79HoYUmPN/9HP2HP3XQF48Qt248prrn/Q+R+dez777Ll7D0YmdU9HApqIuGKY7UoexlMAVR/bbDP9gf2Xv+zFXH/9H3o4GqnZtpi0OZf89koALvrNZfzbVlMfOHfP8ntZ9Nsref5uz+7V8NTHuvwup47q1ByaycCLgTtXaw/gVx26p3rk61/7As977rOZNGkz/nTDIj7y0ePYe+8XsN12T2BwcJCbblrKIbNc4SS1w7uPOoZLfnsFd931V/Z4xf4ccvDr+ciRh3LMCV9mxcqVrL/eehx1xKEPXH/u+b/iOTvvyIaP3KCHo1a/GmzQgo2OLNuOiJOBr2bmhWs4943MfO1ofbhsW+oNl21LvdPtZduv/7dXte137dduPLuny7Y7kqHJzINHODdqMCNJkjqvSZkDl21LklSoMbyDqTZ8sJ4kSao9MzSSJBWqH54f0y4GNJIkFaofllu3iyUnSZJUe2ZoJEkqVJMmBRvQSJJUqCbNobHkJEmSas8MjSRJhWrSpGADGkmSCtWJ1x/1iiUnSZJUe2ZoJEkqlKucJElS7TmHRpIk1Z7LtiVJkvqIGRpJkgrlHBpJklR7LtuWJEnqI2ZoJEkqlKucJElS7bnKSZIkqY+YoZEkqVCucpIkSbXnKidJkqQ+YoZGkqRCWXKSJEm15yonSZKkPmJAI0lSoQYz27aNRUSsGxG/jYjvV8fTI+KiiFgcEWdExHrj/S4GNJIkFSrbuI3RO4BrW46PBY7PzG2AO4GDx/tdDGgkSVLHRcQ04CXASdVxAC8AzqoumQu8Yrz9OylYkqRCtXOVU0QMAAMtTbMzc3bL8X8DRwAbV8ebA3dl5orqeAkwdbz3N6CRJKlQ7QxoquBl9prORcRLgWWZ+ZuI2L1tN21hQCNJkjptV+DlEbEPsAHwaOAEYGJETKiyNNOApeO9gXNoJEkqVGa2bRvlPu/NzGmZuTWwL/DzzHwd8Avg1dVlBwLfHe93MaCRJKlQg2TbtnE6Ejg8IhYzNKfm5PF2ZMlJkiR1TWaeB5xX7d8A7NyOfg1oJEkqVJNefWBAI0lSoUab+1InzqGRJEm1Z4ZGkqRCtfM5NL1mQCNJUqEsOUmSJPURMzSSJBXKkpMkSaq9Ji3btuQkSZJqzwyNJEmFGmzQpGADGkmSCmXJSZIkqY+YoZEkqVCWnCRJUu1ZcpIkSeojZmgkSSqUJSdJklR7lpwkSZL6iBkaSZIKZclJkiTVniUnSZKkPmKGRpKkQmUO9noIbWNAI0lSoQYtOUmSJPUPMzSSJBUqXeUkSZLqzpKTJElSHzFDI0lSoSw5SZKk2mvSk4ItOUmSpNozQyNJUqGa9OoDAxpJkgrlHBpJklR7LtuWJEnqIwY0kiQVKjPbto0kIraKiF9ExDURcXVEvKNq3ywizomI31d/bjre72JAI0lSoQYz27aNYgXwX5k5A9gFmBURM4D3AOdm5rbAudXxuBjQSJKkjsrMWzLz0mr/HuBaYCowE5hbXTYXeMV47+GkYEmSCtXOVU4RMQAMtDTNzszZa7hua+AZwEXA5My8pTr1F2DyeO9vQCNJUqHaucqpCl4eEsC0iohHAd8C3pmZf42I1s9nRIx7QJacJElSx0XEIxgKZk7LzLOr5lsjYkp1fgqwbLz9G9BIklSoLq5yCuBk4NrM/GzLqfnAgdX+gcB3x/tdLDlJklSoLr6cclfg9cCVEXFZ1fY+4BjgzIg4GLgReM14b2BAI0mSOiozLwRimNN7tOMeBjSSJBXKl1NKkqTa62LJqeOcFCxJkmrPDI0kSYVq54P1es2ARpKkQjVpDo0lJ0mSVHtmaCRJKpQlJ0mSVHtNCmgsOUmSpNozQyNJUqGak5+BaFK6Sf0jIgaqV8lL6iJ/9lQqS07qlIFeD0AqlD97KpIBjSRJqj0DGkmSVHsGNOoUa/hSb/izpyI5KViSJNWeGRpJklR7BjSSJKn2DGjUVhGxV0RcHxGLI+I9vR6PVIqImBMRyyLiql6PReoFAxq1TUSsC3wB2BuYAewXETN6OyqpGKcAe/V6EFKvGNConXYGFmfmDZn5D+B0YGaPxyQVITMXAHf0ehxSrxjQqJ2mAje3HC+p2iRJ6igDGkmSVHsGNGqnpcBWLcfTqjZJkjrKgEbtdAmwbURMj4j1gH2B+T0ekySpAAY0apvMXAG8DfgJcC1wZmZe3dtRSWWIiHnAr4HtI2JJRBzc6zFJ3eSrDyRJUu2ZoZEkSbVnQCNJkmrPgEaSJNWeAY0kSao9AxpJklR7BjRSTUXEyoi4LCKuiohvRsSGD6OvUyLi1dX+SSO9VDQido+I54zjHn+KiEnjHaMkjcSARqqvv2XmDpn5FOAfwFtaT0bEhPF0mplvzMxrRrhkd2CtAxpJ6iQDGqkZLgC2qbInF0TEfOCaiFg3Ij4dEZdExBUR8WaAGHJiRFwfET8DtlzVUUScFxE7Vft7RcSlEXF5RJwbEVszFDgdVmWHdouILSLiW9U9LomIXavPbh4RP42IqyPiJCC6+1ciqSTj+hecpP5RZWL2Bn5cNe0IPCUz/xgRA8DdmfnMiFgf+GVE/BR4BrA9MAOYDFwDzFmt3y2ArwDPrfraLDPviIj/AZZn5nHVdd8Ajs/MCyPicQw9KfpJwFHAhZn50Yh4CeCTayV1jAGNVF+PjIjLqv0LgJMZKgVdnJl/rNpfBDxt1fwYYBNgW+C5wLzMXAn8OSJ+vob+dwEWrOorM+8YZhx7AjMiHkjAPDoiHlXd41XVZ38QEXeO83tK0qgMaKT6+ltm7tDaUAUV97Y2AW/PzJ+sdt0+bRzHOsAumfn3NYxFkrrCOTRSs/0EeGtEPAIgIraLiI2ABcB/VnNspgDPX8NnFwLPjYjp1Wc3q9rvATZuue6nwNtXHUTEqiBrAfDaqm1vYNO2fStJWo0BjdRsJzE0P+bSiLgK+DJDmdlvA7+vzp3K0FuaHyQzbwMGgLMj4nLgjOrU94BXrpoUDBwK7FRNOr6Gf622+ghDAdHVDJWeburQd5Qk37YtSZLqzwyNJEmqPQMaSZJUewY0kiSp9gxoJElS7RnQSJKk2jOgkSRJtWdAI0mSau//A1Kiqb0pc1nbAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}