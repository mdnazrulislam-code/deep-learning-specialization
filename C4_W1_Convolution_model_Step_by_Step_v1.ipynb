{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DepQIj7bgrxJRn_9JStsnswQDzUKSzBm",
      "authorship_tag": "ABX9TyPCfnM4EscWWfc8rDXc4IVD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cSDpbFBDzoe"
      },
      "outputs": [],
      "source": [
        "#necessary files to import....................\n",
        "\n",
        "zero_pad_output0 = [[[[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [1.62434536, -0.61175641],\n",
        "                                                    [-0.52817175, -1.07296862],\n",
        "                                                    [0.86540763, -2.3015387],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [1.74481176, -0.7612069],\n",
        "                                                    [0.3190391, -0.24937038],\n",
        "                                                    [1.46210794, -2.06014071],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [-0.3224172, -0.38405435],\n",
        "                                                    [1.13376944, -1.09989127],\n",
        "                                                    [-0.17242821, -0.87785842],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                    [0., 0.]]],\n",
        "\n",
        "\n",
        "                                                  [[[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0.04221375, 0.58281521],\n",
        "                                                      [-1.10061918, 1.14472371],\n",
        "                                                      [0.90159072, 0.50249434],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0.90085595, -0.68372786],\n",
        "                                                      [-0.12289023, -0.93576943],\n",
        "                                                      [-0.26788808, 0.53035547],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [-0.69166075, -0.39675353],\n",
        "                                                      [-0.6871727, -0.84520564],\n",
        "                                                      [-0.67124613, -0.0126646],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]]],\n",
        "\n",
        "\n",
        "                                                  [[[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [-1.11731035, 0.2344157],\n",
        "                                                      [1.65980218, 0.74204416],\n",
        "                                                      [-0.19183555, -0.88762896],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [-0.74715829, 1.6924546],\n",
        "                                                      [0.05080775, -0.63699565],\n",
        "                                                      [0.19091548, 2.10025514],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0.12015895, 0.61720311],\n",
        "                                                      [0.30017032, -0.35224985],\n",
        "                                                      [-1.1425182, -0.34934272],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]]],\n",
        "\n",
        "\n",
        "                                                  [[[0., 0.],\n",
        "                                                    [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [-0.20889423, 0.58662319],\n",
        "                                                      [0.83898341, 0.93110208],\n",
        "                                                      [0.28558733, 0.88514116],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [-0.75439794, 1.25286816],\n",
        "                                                      [0.51292982, -0.29809284],\n",
        "                                                      [0.48851815, -0.07557171],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [1.13162939, 1.51981682],\n",
        "                                                      [2.18557541, -1.39649634],\n",
        "                                                      [-1.44411381, -0.50446586],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]],\n",
        "\n",
        "                                                   [[0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.],\n",
        "                                                      [0., 0.]]]]\n",
        "conv_forward_output0 = [[[[-2.65112363, -0.37849177, -1.97054929, -1.96235299,\n",
        "                              -1.72259872, 0.4676693, -6.43434016, 1.10764994],\n",
        "                             [4.67692928, 4.29865415, -1.3608031, 0.80532859,\n",
        "                              -2.88480108, 8.95280034, 5.32627807, -1.82635258],\n",
        "                             [-2.05881174, 3.40859795, 0.3502282, 0.68303626,\n",
        "                              -1.88328065, -1.87480174, 5.8008721, 0.0700918],\n",
        "                             [-3.50141791, 2.704286, 0.28341346, 4.15637411,\n",
        "                              -0.46575834, -0.43668824, -5.56866106, 1.72288033]],\n",
        "\n",
        "                            [[-2.32126108, 0.91040602, 2.31852532, 0.98842271,\n",
        "                              3.31716611, 4.05638832, -2.48135123, 0.95872443],\n",
        "                             [6.03978907, -6.96477888, -1.20799344, 2.68913374,\n",
        "                                -4.35744033, 10.59355329, 3.20856901, 13.98735978],\n",
        "                             [-3.01280755, -2.90226517, -8.34171936, -5.26220853,\n",
        "                              5.6630696, 1.08704033, 2.20430705, -10.73218294],\n",
        "                             [-6.24198266, -0.53158832, -3.29654954, -1.81865997,\n",
        "                              0.59196322, 2.51134745, -4.24924673, 5.21936641]],\n",
        "\n",
        "                            [[-2.22187412, -0.95259173, -5.99441273, 0.79147932,\n",
        "                              1.16919278, -0.17321161, -3.26346299, -3.62407578],\n",
        "                             [-2.17796037, 8.07171329, -0.5772704, 3.36286738,\n",
        "                              4.48113645, -2.89198428, 10.99288867, 3.03171932],\n",
        "                             [-12.49991261, 5.26845833, -1.67648614, -8.65695762,\n",
        "                                -10.68157258, 6.71492428, 2.83839971, 4.47259772],\n",
        "                             [0.11421092, -1.90872424, -3.28117601, 0.89922467,\n",
        "                              0.83985348, -0.25127044, -0.94409718, 5.17244412]]],\n",
        "\n",
        "\n",
        "                           [[[1.97649814, 2.76743075, -6.39611007, 2.95378171,\n",
        "                              -0.81235239, -0.53333631, 0.71268871, 4.91385105],\n",
        "                             [-5.14401869, 6.97041391, -4.53976469, 5.89092653,\n",
        "                               -5.74606931, 2.74256558, 3.02124802, -10.04187592],\n",
        "                               [5.53871187, -8.55886701, -4.70962135, 2.55966738,\n",
        "                                -2.66959504, 5.60010695, -8.37253342, 4.18848278],\n",
        "                               [0.63364517, -3.71848223, -3.67072772, 4.34226476,\n",
        "                                -1.21894465, 3.68929452, 5.89166305, 0.94256457]],\n",
        "\n",
        "                            [[2.36049402, -3.09696204, 8.33521755, 3.04680748,\n",
        "                              3.7964542, 0.66488788, 1.9935476, 1.54396221],\n",
        "                               [-7.73457048, 0.287562, 7.97481218, 3.32415996,\n",
        "                                -4.07121488, 2.69182963, 4.1356109, -5.16178423],\n",
        "                               [-6.95635186, -0.10924121, -4.12526441, 0.62578199,\n",
        "                                4.69492086, -3.52748877, 3.63168271, 0.64007629],\n",
        "                               [7.94980014, 5.71855659, 3.49970333, 12.7718152,\n",
        "                                8.84959478, 2.37150319, -1.42531648, -0.51126641]],\n",
        "\n",
        "                            [[-5.29658283, -4.20466999, -6.63067766, -9.87831724,\n",
        "                              -5.32130395, 7.32417919, 2.96011091, 7.60669481],\n",
        "                               [11.54630784, -1.93157244, 2.26699242, 7.62184275,\n",
        "                                5.40584348, -2.88837958, -1.46981877, 7.91314719],\n",
        "                               [5.94067877, 3.50739649, 0.82512202, 4.80655489,\n",
        "                                -4.1044945, 4.14358541, 0.13194885, 4.35397285],\n",
        "                               [4.91298364, -1.44499772, 5.9392078, -3.92690408,\n",
        "                                2.12840309, 1.27237402, 1.56992581, 0.44270565]]]]\n",
        "\n",
        "pool_forward_output0 = [[[[1.74481176, 0.90159072, 1.65980218],\n",
        "                                 [1.74481176, 1.46210794, 1.65980218],\n",
        "                                 [1.74481176, 1.6924546, 1.65980218]],\n",
        "\n",
        "                                [[1.14472371, 0.90159072, 2.10025514],\n",
        "                                 [1.14472371, 0.90159072, 1.65980218],\n",
        "                                 [1.14472371, 1.6924546, 1.65980218]],\n",
        "\n",
        "                                [[1.13162939, 1.51981682, 2.18557541],\n",
        "                                 [1.13162939, 1.51981682, 2.18557541],\n",
        "                                 [1.13162939, 1.6924546, 2.18557541]]],\n",
        "\n",
        "\n",
        "                               [[[1.19891788, 0.84616065, 0.82797464],\n",
        "                                 [0.69803203, 0.84616065, 1.2245077],\n",
        "                                   [0.69803203, 1.12141771, 1.2245077]],\n",
        "\n",
        "                                [[1.96710175, 0.84616065, 1.27375593],\n",
        "                                   [1.96710175, 0.84616065, 1.23616403],\n",
        "                                   [1.62765075, 1.12141771, 1.2245077]],\n",
        "\n",
        "                                [[1.96710175, 0.86888616, 1.27375593],\n",
        "                                   [1.96710175, 0.86888616, 1.23616403],\n",
        "                                   [1.62765075, 1.12141771, 0.79280687]]]]\n",
        "\n",
        "pool_forward_output1 = [[[[-3.01046719e-02, -3.24021315e-03, -3.36298859e-01],\n",
        "                                     [1.43310483e-01, 1.93146751e-01, -\n",
        "                                      4.44905196e-01],\n",
        "                                     [1.28934436e-01, 2.22428468e-01, 1.25067597e-01]],\n",
        "\n",
        "                                    [[-3.81801899e-01, 1.59993515e-02, 1.70562706e-01],\n",
        "                                     [4.73707165e-02, 2.59244658e-02,\n",
        "                                        9.20338402e-02],\n",
        "                                     [3.97048605e-02, 1.57189094e-01, 3.45302489e-01]],\n",
        "\n",
        "                                    [[-3.82680519e-01, 2.32579951e-01, 6.25997903e-01],\n",
        "                                     [-2.47157416e-01, -3.48524998e-04,\n",
        "                                      3.50539717e-01],\n",
        "                                     [-9.52551510e-02, 2.68511000e-01, 4.66056368e-01]]],\n",
        "\n",
        "\n",
        "                                   [[[-1.73134159e-01, 3.23771981e-01, -3.43175716e-01],\n",
        "                                     [3.80634669e-02, 7.26706274e-02, -\n",
        "                                      2.30268958e-01],\n",
        "                                       [2.03009393e-02, 1.41414785e-01, -1.23158476e-02]],\n",
        "\n",
        "                                    [[4.44976963e-01, -2.61694592e-03, -3.10403073e-01],\n",
        "                                       [5.08114737e-01, -\n",
        "                                        2.34937338e-01, -2.39611830e-01],\n",
        "                                       [1.18726772e-01, 1.72552294e-01, -2.21121966e-01]],\n",
        "\n",
        "                                    [[4.29449255e-01, 8.44699612e-02, -2.72909051e-01],\n",
        "                                       [6.76351685e-01, -\n",
        "                                        1.20138225e-01, -2.44076712e-01],\n",
        "                                       [1.50774518e-01, 2.89111751e-01, 1.23238536e-03]]]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def datatype_check(expected_output, target_output, error):\n",
        "    success = 0\n",
        "    if isinstance(target_output, dict):\n",
        "        for key in target_output.keys():\n",
        "            try:\n",
        "                success += datatype_check(expected_output[key],\n",
        "                                          target_output[key], error)\n",
        "            except:\n",
        "                print(\"Error: {} in variable {}. Got {} but expected type {}\".format(error,\n",
        "                                                                                     key, type(target_output[key]), type(expected_output[key])))\n",
        "        if success == len(target_output.keys()):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n",
        "        for i in range(len(target_output)):\n",
        "            try:\n",
        "                success += datatype_check(expected_output[i],\n",
        "                                          target_output[i], error)\n",
        "            except:\n",
        "                print(\"Error: {} in variable {}, expected type: {}  but expected type {}\".format(error,\n",
        "                                                                                                 i, type(target_output[i]), type(expected_output[i])))\n",
        "        if success == len(target_output):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    else:\n",
        "        assert isinstance(target_output, type(expected_output))\n",
        "        return 1\n",
        "\n",
        "\n",
        "def equation_output_check(expected_output, target_output, error):\n",
        "    success = 0\n",
        "    if isinstance(target_output, dict):\n",
        "        for key in target_output.keys():\n",
        "            try:\n",
        "                success += equation_output_check(expected_output[key],\n",
        "                                                 target_output[key], error)\n",
        "            except:\n",
        "                print(\"Error: {} for variable {}.\".format(error,\n",
        "                                                          key))\n",
        "        if success == len(target_output.keys()):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n",
        "        for i in range(len(target_output)):\n",
        "            try:\n",
        "                success += equation_output_check(expected_output[i],\n",
        "                                                 target_output[i], error)\n",
        "            except:\n",
        "                print(\"Error: {} for variable in position {}.\".format(error, i))\n",
        "        if success == len(target_output):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    else:\n",
        "        if hasattr(target_output, 'shape'):\n",
        "            np.testing.assert_array_almost_equal(\n",
        "                target_output, expected_output)\n",
        "        else:\n",
        "            assert target_output == expected_output\n",
        "        return 1\n",
        "\n",
        "\n",
        "def shape_check(expected_output, target_output, error):\n",
        "    success = 0\n",
        "    if isinstance(target_output, dict):\n",
        "        for key in target_output.keys():\n",
        "            try:\n",
        "                success += shape_check(expected_output[key],\n",
        "                                       target_output[key], error)\n",
        "            except:\n",
        "                print(\"Error: {} for variable {}.\".format(error, key))\n",
        "        if success == len(target_output.keys()):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n",
        "        for i in range(len(target_output)):\n",
        "            try:\n",
        "                success += shape_check(expected_output[i],\n",
        "                                       target_output[i], error)\n",
        "            except:\n",
        "                print(\"Error: {} for variable {}.\".format(error, i))\n",
        "        if success == len(target_output):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    else:\n",
        "        if hasattr(target_output, 'shape'):\n",
        "            assert target_output.shape == expected_output.shape\n",
        "        return 1\n",
        "\n",
        "\n",
        "def single_test(test_cases, target):\n",
        "    success = 0\n",
        "    for test_case in test_cases:\n",
        "        try:\n",
        "            if test_case['name'] == \"datatype_check\":\n",
        "                assert isinstance(target(*test_case['input']),\n",
        "                                  type(test_case[\"expected\"]))\n",
        "                success += 1\n",
        "            if test_case['name'] == \"equation_output_check\":\n",
        "                assert np.allclose(test_case[\"expected\"],\n",
        "                                   target(*test_case['input']))\n",
        "                success += 1\n",
        "            if test_case['name'] == \"shape_check\":\n",
        "                assert test_case['expected'].shape == target(\n",
        "                    *test_case['input']).shape\n",
        "                success += 1\n",
        "        except:\n",
        "            print(\"Error: \" + test_case['error'])\n",
        "\n",
        "    if success == len(test_cases):\n",
        "        print(\"\\033[92m All tests passed.\")\n",
        "    else:\n",
        "        print('\\033[92m', success, \" Tests passed\")\n",
        "        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n",
        "        raise AssertionError(\n",
        "            \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n",
        "\n",
        "\n",
        "def multiple_test(test_cases, target):\n",
        "    success = 0\n",
        "    for test_case in test_cases:\n",
        "        try:\n",
        "            target_answer = target(*test_case['input'])\n",
        "            if test_case['name'] == \"datatype_check\":\n",
        "                success += datatype_check(test_case['expected'],\n",
        "                                          target_answer, test_case['error'])\n",
        "            if test_case['name'] == \"equation_output_check\":\n",
        "                success += equation_output_check(\n",
        "                    test_case['expected'], target_answer, test_case['error'])\n",
        "            if test_case['name'] == \"shape_check\":\n",
        "                success += shape_check(test_case['expected'],\n",
        "                                       target_answer, test_case['error'])\n",
        "        except:\n",
        "            print(\"Error: \" + test_case['error'])\n",
        "\n",
        "    if success == len(test_cases):\n",
        "        print(\"\\033[92m All tests passed.\")\n",
        "    else:\n",
        "        print('\\033[92m', success, \" Tests passed\")\n",
        "        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n",
        "        raise AssertionError(\n",
        "            \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n"
      ],
      "metadata": {
        "id": "q2_f65uzw1GX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def zero_pad_test(target):    \n",
        "    # Test 1\n",
        "    np.random.seed(1)\n",
        "    x = np.random.randn(4, 3, 3, 2)\n",
        "    x_pad = target(x, 3)\n",
        "    print (\"x.shape =\\n\", x.shape)\n",
        "    print (\"x_pad.shape =\\n\", x_pad.shape)\n",
        "    print (\"x[1,1] =\\n\", x[1, 1])\n",
        "    print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
        "\n",
        "    assert type(x_pad) == np.ndarray, \"Output must be a np array\"\n",
        "    assert x_pad.shape == (4, 9, 9, 2), f\"Wrong shape: {x_pad.shape} != (4, 9, 9, 2)\"\n",
        "    print(x_pad[0, 0:2,:, 0])\n",
        "    assert np.allclose(x_pad[0, 0:2,:, 0], [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Rows are not padded with zeros\"\n",
        "    assert np.allclose(x_pad[0, :, 7:9, 1].transpose(), [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Columns are not padded with zeros\"\n",
        "    assert np.allclose(x_pad[:, 3:6, 3:6, :], x, 1e-15), \"Internal values are different\"\n",
        "\n",
        "    # Test 2\n",
        "    np.random.seed(1)\n",
        "    x = np.random.randn(5, 4, 4, 3)\n",
        "    pad = 2\n",
        "    x_pad = target(x, pad)\n",
        "    \n",
        "    assert type(x_pad) == np.ndarray, \"Output must be a np array\"\n",
        "    assert x_pad.shape == (5, 4 + 2 * pad, 4 + 2 * pad, 3), f\"Wrong shape: {x_pad.shape} != {(5, 4 + 2 * pad, 4 + 2 * pad, 3)}\"\n",
        "    assert np.allclose(x_pad[0, 0:2,:, 0], [[0, 0, 0, 0, 0, 0, 0, 0], \n",
        "                                            [0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Rows are not padded with zeros\"\n",
        "    assert np.allclose(x_pad[0, :, 6:8, 1].transpose(), [[0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                                         [0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Columns are not padded with zeros\"\n",
        "    assert np.allclose(x_pad[:, 2:6, 2:6, :], x, 1e-15), \"Internal values are different\"\n",
        "    \n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "    \n",
        "\n",
        "\n",
        "def conv_single_step_test(target):\n",
        "\n",
        "    np.random.seed(3)\n",
        "    a_slice_prev = np.random.randn(5, 5, 3)\n",
        "    W = np.random.randn(5, 5, 3)\n",
        "    b = np.random.randn(1, 1, 1)\n",
        "    \n",
        "    Z = target(a_slice_prev, W, b)\n",
        "    expected_output = np.float64(-3.5443670581382474)\n",
        "    \n",
        "    assert (type(Z) == np.float64 or type(Z) == np.float32), \"You must cast the output to float\"\n",
        "    assert np.isclose(Z, expected_output), f\"Wrong value. Expected: {expected_output} got: {Z}\"\n",
        "    \n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "\n",
        "def conv_forward_test_1(z_mean, z_0_2_1, cache_0_1_2_3):\n",
        "    test_count = 0\n",
        "    z_mean_expected = 0.5511276474566768\n",
        "    z_0_2_1_expected = [-2.17796037,  8.07171329, -0.5772704,   3.36286738,  4.48113645, -2.89198428, 10.99288867,  3.03171932]\n",
        "    cache_0_1_2_3_expected = [-1.1191154,   1.9560789,  -0.3264995,  -1.34267579]\n",
        "    \n",
        "    if np.isclose(z_mean, z_mean_expected):\n",
        "        test_count = test_count + 1\n",
        "    else:\n",
        "        print(\"\\033[91mFirst Test: Z's mean is incorrect. Expected:\", z_mean_expected, \"\\nYour output:\", z_mean, \"\\033[90m\\n\")\n",
        "        \n",
        "    if np.allclose(z_0_2_1, z_0_2_1_expected):\n",
        "        test_count = test_count + 1\n",
        "    else:\n",
        "        print(\"\\033[91mFirst Test: Z[0,2,1] is incorrect. Expected:\", z_0_2_1_expected, \"\\nYour output:\", z_0_2_1, \"\\033[90m\\n\")\n",
        "        \n",
        "    if np.allclose(cache_0_1_2_3, cache_0_1_2_3_expected):\n",
        "        test_count = test_count + 1\n",
        "    else:\n",
        "        print(\"\\033[91mFirst Test: cache_conv[0][1][2][3] is incorrect. Expected:\", cache_0_1_2_3_expected, \"\\nYour output:\",\n",
        "              cache_0_1_2_3, \"\\033[90m\")\n",
        "    \n",
        "    if test_count == 3:\n",
        "        print(\"\\033[92mFirst Test: All tests passed!\")\n",
        "    \n",
        "def conv_forward_test_2(target):\n",
        "    # Test 1\n",
        "    np.random.seed(3)\n",
        "    A_prev = np.random.randn(2, 5, 7, 4)\n",
        "    W = np.random.randn(3, 3, 4, 8)\n",
        "    b = np.random.randn(1, 1, 1, 8)\n",
        "    \n",
        "    Z, cache_conv = target(A_prev, W, b, {\"pad\" : 3, \"stride\": 1})\n",
        "    Z_shape = Z.shape\n",
        "    assert Z_shape[0] == A_prev.shape[0], f\"m is wrong. Current: {Z_shape[0]}.  Expected: {A_prev.shape[0]}\"\n",
        "    assert Z_shape[1] == 9, f\"n_H is wrong. Current: {Z_shape[1]}.  Expected: 9\"\n",
        "    assert Z_shape[2] == 11, f\"n_W is wrong. Current: {Z_shape[2]}.  Expected: 11\"\n",
        "    assert Z_shape[3] == W.shape[3], f\"n_C is wrong. Current: {Z_shape[3]}.  Expected: {W.shape[3]}\"\n",
        "\n",
        "    # Test 2 \n",
        "    Z, cache_conv = target(A_prev, W, b, {\"pad\" : 0, \"stride\": 2})\n",
        "    assert(Z.shape == (2, 2, 3, 8)), \"Wrong shape. Don't hard code the pad and stride values in the function\"\n",
        "    \n",
        "    # Test 3\n",
        "    W = np.random.randn(5, 5, 4, 8)\n",
        "    b = np.random.randn(1, 1, 1, 8)\n",
        "    Z, cache_conv = target(A_prev, W, b, {\"pad\" : 6, \"stride\": 1})\n",
        "    Z_shape = Z.shape\n",
        "    assert Z_shape[0] == A_prev.shape[0], f\"m is wrong. Current: {Z_shape[0]}.  Expected: {A_prev.shape[0]}\"\n",
        "    assert Z_shape[1] == 13, f\"n_H is wrong. Current: {Z_shape[1]}.  Expected: 13\"\n",
        "    assert Z_shape[2] == 15, f\"n_W is wrong. Current: {Z_shape[2]}.  Expected: 15\"\n",
        "    assert Z_shape[3] == W.shape[3], f\"n_C is wrong. Current: {Z_shape[3]}.  Expected: {W.shape[3]}\"\n",
        "\n",
        "    Z_means = np.mean(Z)\n",
        "    expected_Z = -0.5384027772160062\n",
        "    \n",
        "    expected_conv = np.array([[ 1.98848968,  1.19505834, -0.0952376,  -0.52718778],\n",
        "                             [-0.32158469,  0.15113037, -0.01862772,  0.48352879],\n",
        "                             [ 0.76896516,  1.36624284,  1.14726479, -0.11022916],\n",
        "                             [ 0.38825041, -0.38712718, -0.58722031,  1.91082685],\n",
        "                             [-0.45984615,  1.99073781, -0.34903539,  0.25282509],\n",
        "                             [ 1.08940955,  0.02392202,  0.39312528, -0.2413848 ],\n",
        "                             [-0.47552486, -0.16577702, -0.64971742,  1.63138295]])\n",
        "    \n",
        "    assert np.isclose(Z_means, expected_Z), f\"Wrong Z mean. Expected: {expected_Z} got: {Z_means}\"\n",
        "    assert np.allclose(cache_conv[0][1, 2], expected_conv), f\"Values in Z are wrong\"\n",
        "\n",
        "    print(\"\\033[92mSecond Test: All tests passed!\")\n",
        "\n",
        "\n",
        "def pool_forward_test(target):\n",
        "    \n",
        "    # Test 1\n",
        "    A_prev = np.random.randn(2, 5, 7, 3)\n",
        "    A, cache = target(A_prev, {\"stride\" : 2, \"f\": 2}, mode = \"average\")\n",
        "    A_shape = A.shape\n",
        "    assert A_shape[0] == A_prev.shape[0], f\"Test 1 - m is wrong. Current: {A_shape[0]}.  Expected: {A_prev.shape[0]}\"\n",
        "    assert A_shape[1] == 2, f\"Test 1 - n_H is wrong. Current: {A_shape[1]}.  Expected: 2\"\n",
        "    assert A_shape[2] == 3, f\"Test 1 - n_W is wrong. Current: {A_shape[2]}.  Expected: 3\"\n",
        "    assert A_shape[3] == A_prev.shape[3], f\"Test 1 - n_C is wrong. Current: {A_shape[3]}.  Expected: {A_prev.shape[3]}\"\n",
        "    \n",
        "    # Test 2\n",
        "    A_prev = np.random.randn(4, 5, 7, 4)\n",
        "    A, cache = target(A_prev, {\"stride\" : 1, \"f\": 5}, mode = \"max\")\n",
        "    A_shape = A.shape\n",
        "    assert A_shape[0] == A_prev.shape[0], f\"Test 2 - m is wrong. Current: {A_shape[0]}.  Expected: {A_prev.shape[0]}\"\n",
        "    assert A_shape[1] == 1, f\"Test 2 - n_H is wrong. Current: {A_shape[1]}.  Expected: 1\"\n",
        "    assert A_shape[2] == 3, f\"Test 2 - n_W is wrong. Current: {A_shape[2]}.  Expected: 3\"\n",
        "    assert A_shape[3] == A_prev.shape[3], f\"Test 2 - n_C is wrong. Current: {A_shape[3]}.  Expected: {A_prev.shape[3]}\"\n",
        "    \n",
        "    # Test 3\n",
        "    np.random.seed(1)\n",
        "    A_prev = np.random.randn(2, 5, 5, 3)\n",
        "    \n",
        "    A, cache = target(A_prev, {\"stride\" : 1, \"f\": 2}, mode = \"max\")\n",
        "    \n",
        "    assert np.allclose(A[1, 1], np.array([[1.19891788, 0.74055645, 0.07734007],\n",
        "                                         [0.31515939, 0.84616065, 0.07734007],\n",
        "                                         [0.69803203, 0.84616065, 1.2245077 ],\n",
        "                                         [0.69803203, 1.12141771, 1.2245077 ]])), \"Wrong value for A[1, 1]\"\n",
        "                                          \n",
        "    assert np.allclose(cache[0][1, 2], np.array([[ 0.16938243,  0.74055645, -0.9537006 ],\n",
        "                                         [-0.26621851,  0.03261455, -1.37311732],\n",
        "                                         [ 0.31515939,  0.84616065, -0.85951594],\n",
        "                                         [ 0.35054598, -1.31228341, -0.03869551],\n",
        "                                         [-1.61577235,  1.12141771,  0.40890054]])), \"Wrong value for cache\"\n",
        "    \n",
        "    A, cache = target(A_prev, {\"stride\" : 1, \"f\": 2}, mode = \"average\")\n",
        "    \n",
        "    assert np.allclose(A[1, 1], np.array([[ 0.11583785,  0.34545544, -0.6561907 ],\n",
        "                                         [-0.2334108,   0.3364666,  -0.69382351],\n",
        "                                         [ 0.25497093, -0.21741362, -0.07342615],\n",
        "                                         [-0.04092568, -0.01110394,  0.12495022]])), \"Wrong value for A[1, 1]\"\n",
        "\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "######################################\n",
        "############## UNGRADED ##############\n",
        "######################################\n",
        "\n",
        "\n",
        "def conv_backward_test(target):\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"name\": \"datatype_check\",\n",
        "            \"input\": [parameters, cache, X, Y],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\":\"The function should return a numpy array.\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"shape_check\",\n",
        "            \"input\": [parameters, cache, X, Y],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong shape\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"equation_output_check\",\n",
        "            \"input\": [parameters, cache, X, Y],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong output\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    multiple_test(test_cases, target)\n",
        "\n",
        "\n",
        "def create_mask_from_window_test(target):\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"name\": \"datatype_check\",\n",
        "            \"input\": [parameters, grads],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\":\"Data type mismatch\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"shape_check\",\n",
        "            \"input\": [parameters, grads],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong shape\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"equation_output_check\",\n",
        "            \"input\": [parameters, grads],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong output\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    multiple_test(test_cases, target)\n",
        "\n",
        "\n",
        "def distribute_value_test(target):\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"name\": \"datatype_check\",\n",
        "            \"input\": [X, Y, n_h],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\":\"Data type mismatch\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"shape_check\",\n",
        "            \"input\": [X, Y, n_h],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong shape\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"equation_output_check\",\n",
        "            \"input\": [X, Y, n_h],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong output\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    multiple_test(test_cases, target)\n",
        "\n",
        "\n",
        "def pool_backward_test(target):\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"name\": \"datatype_check\",\n",
        "            \"input\": [parameters, X],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\":\"Data type mismatch\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"shape_check\",\n",
        "            \"input\": [parameters, X],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong shape\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"equation_output_check\",\n",
        "            \"input\": [parameters, X],\n",
        "            \"expected\": expected_output,\n",
        "            \"error\": \"Wrong output\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    single_test(test_cases, target)\n"
      ],
      "metadata": {
        "id": "TJHWuurow7YF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLDZuzoLxEAi",
        "outputId": "41543b28-ea39-40b3-9d96-d2999b4e4ab1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: zero_pad\n",
        "\n",
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
        "    \"\"\"\n",
        "    #(≈ 1 line)\n",
        "    # X_pad = None\n",
        "    # YOUR CODE STARTS HERE\n",
        "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)))\n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    return X_pad"
      ],
      "metadata": {
        "id": "OIbsKClExHx6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(4, 3, 3, 2)\n",
        "x_pad = zero_pad(x, 3)\n",
        "print (\"x.shape =\\n\", x.shape)\n",
        "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
        "print (\"x[1,1] =\\n\", x[1, 1])\n",
        "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2)\n",
        "axarr[0].set_title('x')\n",
        "axarr[0].imshow(x[0, :, :, 0])\n",
        "axarr[1].set_title('x_pad')\n",
        "axarr[1].imshow(x_pad[0, :, :, 0])\n",
        "zero_pad_test(zero_pad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "uOJ5PNaFxxcj",
        "outputId": "10de14f8-4102-4d91-9ade-6e56ed09dd29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape =\n",
            " (4, 3, 3, 2)\n",
            "x_pad.shape =\n",
            " (4, 9, 9, 2)\n",
            "x[1,1] =\n",
            " [[ 0.90085595 -0.68372786]\n",
            " [-0.12289023 -0.93576943]\n",
            " [-0.26788808  0.53035547]]\n",
            "x_pad[1,1] =\n",
            " [[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n",
            "x.shape =\n",
            " (4, 3, 3, 2)\n",
            "x_pad.shape =\n",
            " (4, 9, 9, 2)\n",
            "x[1,1] =\n",
            " [[ 0.90085595 -0.68372786]\n",
            " [-0.12289023 -0.93576943]\n",
            " [-0.26788808  0.53035547]]\n",
            "x_pad[1,1] =\n",
            " [[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\u001b[92mAll tests passed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: conv_single_step\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    #(≈ 3 lines of code)\n",
        "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
        "    s = np.multiply(a_slice_prev,W)\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = np.sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    b = np.squeeze(b)\n",
        "    Z = Z + b\n",
        "    # YOUR CODE STARTS HERE\n",
        "    \n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return Z"
      ],
      "metadata": {
        "id": "TBC6MTORxy-z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "a_slice_prev = np.random.randn(4, 4, 3)\n",
        "W = np.random.randn(4, 4, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "\n",
        "Z = conv_single_step(a_slice_prev, W, b)\n",
        "print(\"Z =\", Z)\n",
        "conv_single_step_test(conv_single_step)\n",
        "\n",
        "assert (type(Z) == np.float64), \"You must cast the output to numpy float 64\"\n",
        "assert np.isclose(Z, -6.999089450680221), \"Wrong value\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCElGhJyTyc",
        "outputId": "0b2f3536-7562-4a5d-f8f3-da50693b33d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z = -6.999089450680221\n",
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: conv_forward\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, \n",
        "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve dimensions from W's shape (≈1 line)\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above. \n",
        "    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
        "    n_H = int((n_H_prev-f+2*pad)/stride)+1\n",
        "    n_W = int((n_W_prev-f+2*pad)/stride)+1\n",
        "    \n",
        "    # Initialize the output volume Z with zeros. (≈1 line)\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    \n",
        "    for i in range(m):               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i]               # Select ith training example's padded activation\n",
        "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
        "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "            vert_start = stride * h \n",
        "            vert_end = vert_start  + f\n",
        "            \n",
        "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
        "                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
        "                horiz_start = stride * w\n",
        "                horiz_end = horiz_start + f\n",
        "                \n",
        "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
        "                                        \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
        "                    weights = W[:, :, :, c]\n",
        "                    biases  = b[:, :, :, c]\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "    # YOUR CODE STARTS HERE\n",
        "    \n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "GUl8bcDjyVmM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 7, 4)\n",
        "W = np.random.randn(3, 3, 4, 8)\n",
        "b = np.random.randn(1, 1, 1, 8)\n",
        "hparameters = {\"pad\" : 1,\n",
        "               \"stride\": 2}\n",
        "\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "z_mean = np.mean(Z)\n",
        "z_0_2_1 = Z[0, 2, 1]\n",
        "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
        "print(\"Z's mean =\\n\", z_mean)\n",
        "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
        "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)\n",
        "\n",
        "conv_forward_test_1(z_mean, z_0_2_1, cache_0_1_2_3)\n",
        "conv_forward_test_2(conv_forward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HIfqAyKzFAd",
        "outputId": "3cb60231-6599-4b52-efb5-3b542ca84caf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z's mean =\n",
            " 0.5511276474566768\n",
            "Z[0,2,1] =\n",
            " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
            " 10.99288867  3.03171932]\n",
            "cache_conv[0][1][2][3] =\n",
            " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n",
            "\u001b[92mFirst Test: All tests passed!\n",
            "\u001b[92mSecond Test: All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: pool_forward\n",
        "\n",
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the forward pass of the pooling layer\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from the input shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve hyperparameters from \"hparameters\"\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = hparameters[\"stride\"]\n",
        "    \n",
        "    # Define the dimensions of the output\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "    \n",
        "    # Initialize output matrix A\n",
        "    A = np.zeros((m, n_H, n_W, n_C))              \n",
        "    \n",
        "    for i in range(m):                         # loop over the training examples\n",
        "        a_prev_slice = A_prev[i]\n",
        "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
        "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "            vert_start = stride * h \n",
        "            vert_end = vert_start + f\n",
        "            \n",
        "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
        "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "                horiz_start = stride * w\n",
        "                horiz_end = horiz_start + f\n",
        "                \n",
        "                for c in range (n_C):            # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "                    a_slice_prev = a_prev_slice[vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "                    \n",
        "                    # Compute the pooling operation on the slice. \n",
        "                    # Use an if statement to differentiate the modes. \n",
        "                    # Use np.max and np.mean.\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
        "                    else:\n",
        "                        print(mode+ \"-type pooling layer NOT Defined\")    \n",
        "    \n",
        "    # YOUR CODE STARTS HERE\n",
        "    \n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "    \n",
        "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
        "    cache = (A_prev, hparameters)\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    #assert(A.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "zGTdTrrTzG-1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 1: stride of 1\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 5, 3)\n",
        "hparameters = {\"stride\" : 1, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1, 1] =\\n\", A[1, 1])\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1, 1] =\\n\", A[1, 1])\n",
        "\n",
        "pool_forward_test(pool_forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPvagIsOzOcc",
        "outputId": "c75228fe-d7d4-46db-aa88-1e44fc138544"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode = max\n",
            "A.shape = (2, 3, 3, 3)\n",
            "A[1, 1] =\n",
            " [[1.96710175 0.84616065 1.27375593]\n",
            " [1.96710175 0.84616065 1.23616403]\n",
            " [1.62765075 1.12141771 1.2245077 ]]\n",
            "mode = average\n",
            "A.shape = (2, 3, 3, 3)\n",
            "A[1, 1] =\n",
            " [[ 0.44497696 -0.00261695 -0.31040307]\n",
            " [ 0.50811474 -0.23493734 -0.23961183]\n",
            " [ 0.11872677  0.17255229 -0.22112197]]\n",
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 2: stride of 2\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 5, 3)\n",
        "hparameters = {\"stride\" : 2, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(\"mode = max\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[0] =\\n\", A[0])\n",
        "print()\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1] =\\n\", A[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spB9pGWszQFE",
        "outputId": "f7d8dd86-7001-4ddc-88a9-0fe676e84b47"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode = max\n",
            "A.shape = (2, 2, 2, 3)\n",
            "A[0] =\n",
            " [[[1.74481176 0.90159072 1.65980218]\n",
            "  [1.74481176 1.6924546  1.65980218]]\n",
            "\n",
            " [[1.13162939 1.51981682 2.18557541]\n",
            "  [1.13162939 1.6924546  2.18557541]]]\n",
            "\n",
            "mode = average\n",
            "A.shape = (2, 2, 2, 3)\n",
            "A[1] =\n",
            " [[[-0.17313416  0.32377198 -0.34317572]\n",
            "  [ 0.02030094  0.14141479 -0.01231585]]\n",
            "\n",
            " [[ 0.42944926  0.08446996 -0.27290905]\n",
            "  [ 0.15077452  0.28911175  0.00123239]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          numpy array of shape (1, 1, 1, n_C)\n",
        "    \"\"\"    \n",
        "    \n",
        "        \n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\"\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    \n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = np.zeros(A_prev.shape)                          \n",
        "    dW = np.zeros(W.shape)\n",
        "    db = np.zeros(b.shape) # b.shape = [1,1,1,n_C]\n",
        "    \n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "    \n",
        "    for i in range(m):                       # loop over the training examples\n",
        "        \n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "        \n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = stride * h \n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = stride * w\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                    \n",
        "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "Ypc4eP_4zTYk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
        "# which we'll use to test the conv_backward function\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10, 4, 4, 3)\n",
        "W = np.random.randn(2, 2, 3, 8)\n",
        "b = np.random.randn(1, 1, 1, 8)\n",
        "hparameters = {\"pad\" : 2,\n",
        "               \"stride\": 2}\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "\n",
        "# Test conv_backward\n",
        "dA, dW, db = conv_backward(Z, cache_conv)\n",
        "\n",
        "print(\"dA_mean =\", np.mean(dA))\n",
        "print(\"dW_mean =\", np.mean(dW))\n",
        "print(\"db_mean =\", np.mean(db))\n",
        "\n",
        "assert type(dA) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert type(dW) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert type(db) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert dA.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n",
        "assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n",
        "assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n",
        "assert np.isclose(np.mean(dA), 1.4524377), \"Wrong values for dA\"\n",
        "assert np.isclose(np.mean(dW), 1.7269914), \"Wrong values for dW\"\n",
        "assert np.isclose(np.mean(db), 7.8392325), \"Wrong values for db\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPE_ysQ7zysU",
        "outputId": "e5c168ff-140d-452d-9479-d245e7f3f5f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dA_mean = 1.4524377775388075\n",
            "dW_mean = 1.7269914583139097\n",
            "db_mean = 7.839232564616838\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_window(x):\n",
        "    \"\"\"\n",
        "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- Array of shape (f, f)\n",
        "    \n",
        "    Returns:\n",
        "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "    \"\"\"    \n",
        "    # (≈1 line)\n",
        "    # mask = None\n",
        "    # YOUR CODE STARTS HERE\n",
        "    mask = (x == np.max(x))\n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Ec0wX2z3z0Qd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(2, 3)\n",
        "mask = create_mask_from_window(x)\n",
        "print('x = ', x)\n",
        "print(\"mask = \", mask)\n",
        "\n",
        "x = np.array([[-1, 2, 3],\n",
        "              [2, -3, 2],\n",
        "              [1, 5, -2]])\n",
        "\n",
        "y = np.array([[False, False, False],\n",
        "     [False, False, False],\n",
        "     [False, True, False]])\n",
        "mask = create_mask_from_window(x)\n",
        "\n",
        "assert type(mask) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert mask.shape == x.shape, \"Input and output shapes must match\"\n",
        "assert np.allclose(mask, y), \"Wrong output. The True value must be at position (2, 1)\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmd17lVIz310",
        "outputId": "96ea6fa4-8ab7-4b47-de8f-a318b422a9ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
            " [-1.07296862  0.86540763 -2.3015387 ]]\n",
            "mask =  [[ True False False]\n",
            " [False False False]]\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distribute_value(dz, shape):\n",
        "    \"\"\"\n",
        "    Distributes the input value in the matrix of dimension shape\n",
        "    \n",
        "    Arguments:\n",
        "    dz -- input scalar\n",
        "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "    \n",
        "    Returns:\n",
        "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
        "    \"\"\"    \n",
        "    # Retrieve dimensions from shape (≈1 line)\n",
        "    (n_H, n_W) = shape\n",
        "    \n",
        "    # Compute the value to distribute on the matrix (≈1 line)\n",
        "    average = np.prod(shape)\n",
        "    \n",
        "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
        "    a = (dz/average)*np.ones(shape)\n",
        "    # YOUR CODE STARTS HERE\n",
        "    \n",
        "    \n",
        "    # YOUR CODE ENDS HERE\n",
        "    return a"
      ],
      "metadata": {
        "id": "ZzeDUat0z5bE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = distribute_value(2, (2, 2))\n",
        "print('distributed value =', a)\n",
        "\n",
        "\n",
        "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert a.shape == (2, 2), f\"Wrong shape {a.shape} != (2, 2)\"\n",
        "assert np.sum(a) == 2, \"Values must sum to 2\"\n",
        "\n",
        "a = distribute_value(100, (10, 10))\n",
        "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert a.shape == (10, 10), f\"Wrong shape {a.shape} != (10, 10)\"\n",
        "assert np.sum(a) == 100, \"Values must sum to 100\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wINbbKfyz7l1",
        "outputId": "bfe55663-0afa-4e4c-fb19-dd37226516bb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distributed value = [[0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the backward pass of the pooling layer\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
        "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "    \"\"\"\n",
        "    # Retrieve information from cache (≈1 line)\n",
        "    (A_prev, hparameters) = cache\n",
        "    \n",
        "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "    \n",
        "    # Initialize dA_prev with zeros (≈1 line)\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    \n",
        "    for i in range(m): # loop over the training examples\n",
        "        \n",
        "        # select training example from A_prev (≈1 line)\n",
        "        a_prev = A_prev[i,:,:,:]\n",
        "        \n",
        "        for h in range(n_H):                   # loop on the vertical axis\n",
        "            for w in range(n_W):               # loop on the horizontal axis\n",
        "                for c in range(n_C):           # loop over the channels (depth)\n",
        "        \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start  = h * stride\n",
        "                    vert_end    = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end   = w * stride + f\n",
        "                    \n",
        "                    # Compute the backward propagation in both modes.\n",
        "                    if mode == \"max\":\n",
        "                        \n",
        "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
        "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
        "                        \n",
        "                        # Create the mask from a_prev_slice (≈1 line)\n",
        "                        mask = create_mask_from_window( a_prev_slice )\n",
        "\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
        "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
        "                        \n",
        "                    elif mode == \"average\":\n",
        "                        \n",
        "                        # Get the value da from dA (≈2 line)\n",
        "                        da = dA[i, h, w, c]\n",
        "                        \n",
        "                        # Define the shape of the filter as fxf (≈1 line)\n",
        "                        shape = (f,f)\n",
        "\n",
        "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "    \n",
        "    return dA_prev"
      ],
      "metadata": {
        "id": "Q2LIg6Exz_OU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(5, 5, 3, 2)\n",
        "hparameters = {\"stride\" : 1, \"f\": 2}\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(A.shape)\n",
        "print(cache[0].shape)\n",
        "dA = np.random.randn(5, 4, 2, 2)\n",
        "\n",
        "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
        "print()\n",
        "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) \n",
        "\n",
        "assert type(dA_prev1) == np.ndarray, \"Wrong type\"\n",
        "assert dA_prev1.shape == (5, 5, 3, 2), f\"Wrong shape {dA_prev1.shape} != (5, 5, 3, 2)\"\n",
        "assert np.allclose(dA_prev1[1, 1], [[0, 0], \n",
        "                                    [ 5.05844394, -1.68282702],\n",
        "                                    [ 0, 0]]), \"Wrong values for mode max\"\n",
        "assert np.allclose(dA_prev2[1, 1], [[0.08485462,  0.2787552], \n",
        "                                    [1.26461098, -0.25749373], \n",
        "                                    [1.17975636, -0.53624893]]), \"Wrong values for mode average\"\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVaLm_AZ0Hd1",
        "outputId": "17393d40-293d-496f-d0d9-8670b532fa2f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 4, 2, 2)\n",
            "(5, 5, 3, 2)\n",
            "mode = max\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
            " [ 5.05844394 -1.68282702]\n",
            " [ 0.          0.        ]]\n",
            "\n",
            "mode = average\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
            " [ 1.26461098 -0.25749373]\n",
            " [ 1.17975636 -0.53624893]]\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSGARMpl0Jns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}